{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import ContrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = ContrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "Error processing index 389: empty range for randrange() (0, -525939, -525939)\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "Epoch [1/3], Loss: 4.7776\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "Epoch [2/3], Loss: 4.6928\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "Epoch [3/3], Loss: 4.3334\n"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# 학습 - stage 1\n",
    "# ------------\n",
    "\n",
    "\n",
    "\n",
    "#둘 중 하나는 빼고 돌리기 \n",
    "weighting = True\n",
    "if weighting:\n",
    "    train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size)\n",
    "    #staage 2함수\n",
    "else:\n",
    "    train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-755049f409e1>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stage1_model = torch.load(\"NMS_model.pth\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NMS_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)\n\u001b[0;32m---> 27\u001b[0m stage1_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mNMS_model.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m stage1_model\u001b[39m.\u001b[39meval()  \u001b[39m# Stage 1은 학습하지 않음\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# 모델 초기화\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1315'>1316</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1316'>1317</a>\u001b[0m     pickle_load_args[\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1318'>1319</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1319'>1320</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1320'>1321</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1321'>1322</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1322'>1323</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=1323'>1324</a>\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=656'>657</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=657'>658</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=658'>659</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=659'>660</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=660'>661</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=638'>639</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/serialization.py?line=639'>640</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NMS_model.pth'"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Mlp_classifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Hidden Layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid for multi-label classification\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "stage1_model = torch.load(\"NMS_model.pth\")\n",
    "stage1_model.eval()  # Stage 1은 학습하지 않음\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "from models import CotrastiveModel\n",
    "\n",
    "arg = 'sep'\n",
    "# 'sep', 'mean', 'concat'\n",
    "\n",
    "if arg == \"concat\":\n",
    "    input_size = 2*projection_dim\n",
    "else:\n",
    "    input_size = projection_dim\n",
    "\n",
    "\n",
    "hidden_sizes = [128, 64]    # Hidden layer 크기\n",
    "output_size = 3672          # 클래스 수\n",
    "\n",
    "model = Mlp_classifier(input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss 및 Optimizer 설정\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()  # BCE 손실 함수\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------\n",
    "# 학습 - stage 2\n",
    "# ------------\n",
    "num_epochs = 10  # 학습 에포크 수\n",
    "model.to(device)\n",
    "stage1_model.to(device)\n",
    "\n",
    "from sklearn.metrics import hamming_loss  # 해밍 손실 계산\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # 에포크 동안 손실 누적\n",
    "\n",
    "    for batch in train_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_train = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_train.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        \n",
    "        if arg == 'sep' : \n",
    "            # Forward Pass\n",
    "            loss = 0\n",
    "            for emb in [emb1, emb2]:\n",
    "                pred = model(emb)  # MLP로 예측\n",
    "                part_loss = criterion(pred, y_train)  # 손실 계산\n",
    "                loss += part_loss\n",
    "                \n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        elif arg == 'mean':\n",
    "            emb = (emb1 + emb2) / 2\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "        elif arg == 'concat':\n",
    "            emb = torch.cat((emb1, emb2), dim=1)\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid arg value. Choose from 'sep', 'mean', 'concat'.\")\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)  # 평균 손실 계산\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Test (Validation) Loop\n",
    "    val_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred1 = []\n",
    "    y_pred2 = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "        stage1_model.eval()\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_val = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_val.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        y_true.append(y_val.cpu().numpy())  # 리스트에 추가\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if arg == 'sep' : \n",
    "                # Forward Pass\n",
    "                pred1 = model(emb1)\n",
    "                y_pred1.append(pred1.cpu().numpy())  # 예측 결과 저장\n",
    "                pred2 = model(emb2)\n",
    "                y_pred2.append(pred2.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                    \n",
    "            elif arg == 'mean':\n",
    "                emb = (emb1 + emb2) / 2\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                \n",
    "                \n",
    "            elif arg == 'concat':\n",
    "                emb = torch.cat((emb1, emb2), dim=1)\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "    \n",
    "    # 리스트를 NumPy 배열로 변환하여 해밍 손실 계산\n",
    "    y_true = np.concatenate(y_true, axis=0)  # 실제 값\n",
    "    y_pred = np.concatenate(y_pred, axis=0)  # 예측 값\n",
    "\n",
    "    # 'sep'의 경우, pred1과 pred2에 대해 각각 해밍 손실을 계산한 후 평균을 구함\n",
    "    if arg == 'sep':\n",
    "        y_pred1 = np.concatenate(y_pred1, axis=0)\n",
    "        y_pred2 = np.concatenate(y_pred2, axis=0)\n",
    "        score = (hamming_loss(y_true, y_pred1) + hamming_loss(y_true, y_pred2)) / 2\n",
    "\n",
    "    else:\n",
    "        score = hamming_loss(y_true, y_pred)  # 해밍 손실 계산\n",
    "\n",
    "    print(f'Hamming Loss: {score:.4f}')\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1040edb4f031>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CotrastiveModel' from 'models' (/home/elicer/project/src/models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m stage1_model\u001b[39m.\u001b[39meval()  \u001b[39m# Stage 1은 학습하지 않음\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# 모델 초기화\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CotrastiveModel\n\u001b[1;32m     34\u001b[0m arg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msep\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[39m# 'sep', 'mean', 'concat'\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CotrastiveModel' from 'models' (/home/elicer/project/src/models.py)"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Mlp_classifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Hidden Layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid for multi-label classification\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n",
    "stage1_model.eval()  # Stage 1은 학습하지 않음\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "from models import CotrastiveModel\n",
    "\n",
    "arg = 'sep'\n",
    "# 'sep', 'mean', 'concat'\n",
    "\n",
    "if arg == \"concat\":\n",
    "    input_size = 2*projection_dim\n",
    "else:\n",
    "    input_size = projection_dim\n",
    "\n",
    "\n",
    "hidden_sizes = [128, 64]    # Hidden layer 크기\n",
    "output_size = 3672          # 클래스 수\n",
    "\n",
    "model = Mlp_classifier(input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss 및 Optimizer 설정\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()  # BCE 손실 함수\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------\n",
    "# 학습 - stage 2\n",
    "# ------------\n",
    "num_epochs = 10  # 학습 에포크 수\n",
    "model.to(device)\n",
    "stage1_model.to(device)\n",
    "\n",
    "from sklearn.metrics import hamming_loss  # 해밍 손실 계산\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # 에포크 동안 손실 누적\n",
    "\n",
    "    for batch in train_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_train = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_train.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        \n",
    "        if arg == 'sep' : \n",
    "            # Forward Pass\n",
    "            loss = 0\n",
    "            for emb in [emb1, emb2]:\n",
    "                pred = model(emb)  # MLP로 예측\n",
    "                part_loss = criterion(pred, y_train)  # 손실 계산\n",
    "                loss += part_loss\n",
    "                \n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        elif arg == 'mean':\n",
    "            emb = (emb1 + emb2) / 2\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "        elif arg == 'concat':\n",
    "            emb = torch.cat((emb1, emb2), dim=1)\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid arg value. Choose from 'sep', 'mean', 'concat'.\")\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)  # 평균 손실 계산\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Test (Validation) Loop\n",
    "    val_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred1 = []\n",
    "    y_pred2 = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "        stage1_model.eval()\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_val = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_val.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        y_true.append(y_val.cpu().numpy())  # 리스트에 추가\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if arg == 'sep' : \n",
    "                # Forward Pass\n",
    "                pred1 = model(emb1)\n",
    "                y_pred1.append(pred1.cpu().numpy())  # 예측 결과 저장\n",
    "                pred2 = model(emb2)\n",
    "                y_pred2.append(pred2.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                    \n",
    "            elif arg == 'mean':\n",
    "                emb = (emb1 + emb2) / 2\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                \n",
    "                \n",
    "            elif arg == 'concat':\n",
    "                emb = torch.cat((emb1, emb2), dim=1)\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "    \n",
    "    # 리스트를 NumPy 배열로 변환하여 해밍 손실 계산\n",
    "    y_true = np.concatenate(y_true, axis=0)  # 실제 값\n",
    "    y_pred = np.concatenate(y_pred, axis=0)  # 예측 값\n",
    "\n",
    "    # 'sep'의 경우, pred1과 pred2에 대해 각각 해밍 손실을 계산한 후 평균을 구함\n",
    "    if arg == 'sep':\n",
    "        y_pred1 = np.concatenate(y_pred1, axis=0)\n",
    "        y_pred2 = np.concatenate(y_pred2, axis=0)\n",
    "        score = (hamming_loss(y_true, y_pred1) + hamming_loss(y_true, y_pred2)) / 2\n",
    "\n",
    "    else:\n",
    "        score = hamming_loss(y_true, y_pred)  # 해밍 손실 계산\n",
    "\n",
    "    print(f'Hamming Loss: {score:.4f}')\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f55a90f45673>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'projection_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     input_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mprojection_dim\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     input_size \u001b[39m=\u001b[39m projection_dim\n\u001b[1;32m     43\u001b[0m hidden_sizes \u001b[39m=\u001b[39m [\u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m]    \u001b[39m# Hidden layer 크기\u001b[39;00m\n\u001b[1;32m     44\u001b[0m output_size \u001b[39m=\u001b[39m \u001b[39m3672\u001b[39m          \u001b[39m# 클래스 수\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'projection_dim' is not defined"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Mlp_classifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Hidden Layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid for multi-label classification\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n",
    "stage1_model.eval()  # Stage 1은 학습하지 않음\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "from models import ContrastiveModel\n",
    "\n",
    "arg = 'sep'\n",
    "# 'sep', 'mean', 'concat'\n",
    "\n",
    "if arg == \"concat\":\n",
    "    input_size = 2*projection_dim\n",
    "else:\n",
    "    input_size = projection_dim\n",
    "\n",
    "\n",
    "hidden_sizes = [128, 64]    # Hidden layer 크기\n",
    "output_size = 3672          # 클래스 수\n",
    "\n",
    "model = Mlp_classifier(input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss 및 Optimizer 설정\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()  # BCE 손실 함수\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------\n",
    "# 학습 - stage 2\n",
    "# ------------\n",
    "num_epochs = 10  # 학습 에포크 수\n",
    "model.to(device)\n",
    "stage1_model.to(device)\n",
    "\n",
    "from sklearn.metrics import hamming_loss  # 해밍 손실 계산\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # 에포크 동안 손실 누적\n",
    "\n",
    "    for batch in train_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_train = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_train.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        \n",
    "        if arg == 'sep' : \n",
    "            # Forward Pass\n",
    "            loss = 0\n",
    "            for emb in [emb1, emb2]:\n",
    "                pred = model(emb)  # MLP로 예측\n",
    "                part_loss = criterion(pred, y_train)  # 손실 계산\n",
    "                loss += part_loss\n",
    "                \n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        elif arg == 'mean':\n",
    "            emb = (emb1 + emb2) / 2\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "        elif arg == 'concat':\n",
    "            emb = torch.cat((emb1, emb2), dim=1)\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid arg value. Choose from 'sep', 'mean', 'concat'.\")\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)  # 평균 손실 계산\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Test (Validation) Loop\n",
    "    val_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred1 = []\n",
    "    y_pred2 = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "        stage1_model.eval()\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_val = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_val.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        y_true.append(y_val.cpu().numpy())  # 리스트에 추가\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if arg == 'sep' : \n",
    "                # Forward Pass\n",
    "                pred1 = model(emb1)\n",
    "                y_pred1.append(pred1.cpu().numpy())  # 예측 결과 저장\n",
    "                pred2 = model(emb2)\n",
    "                y_pred2.append(pred2.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                    \n",
    "            elif arg == 'mean':\n",
    "                emb = (emb1 + emb2) / 2\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                \n",
    "                \n",
    "            elif arg == 'concat':\n",
    "                emb = torch.cat((emb1, emb2), dim=1)\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "    \n",
    "    # 리스트를 NumPy 배열로 변환하여 해밍 손실 계산\n",
    "    y_true = np.concatenate(y_true, axis=0)  # 실제 값\n",
    "    y_pred = np.concatenate(y_pred, axis=0)  # 예측 값\n",
    "\n",
    "    # 'sep'의 경우, pred1과 pred2에 대해 각각 해밍 손실을 계산한 후 평균을 구함\n",
    "    if arg == 'sep':\n",
    "        y_pred1 = np.concatenate(y_pred1, axis=0)\n",
    "        y_pred2 = np.concatenate(y_pred2, axis=0)\n",
    "        score = (hamming_loss(y_true, y_pred1) + hamming_loss(y_true, y_pred2)) / 2\n",
    "\n",
    "    else:\n",
    "        score = hamming_loss(y_true, y_pred)  # 해밍 손실 계산\n",
    "\n",
    "    print(f'Hamming Loss: {score:.4f}')\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-076bdbb53538>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     72\u001b[0m     clip_a, clip_b, file_ids, target_value \u001b[39m=\u001b[39m batch  \u001b[39m# 오디오와 file_ids 로드\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     clip_a, clip_b, target_value \u001b[39m=\u001b[39m clip_a\u001b[39m.\u001b[39mto(device), clip_b\u001b[39m.\u001b[39mto(device), target_value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     75\u001b[0m     \u001b[39m# Stage 1 Embedding\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Stage 1은 학습하지 않음\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Mlp_classifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Hidden Layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid for multi-label classification\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n",
    "stage1_model.eval()  # Stage 1은 학습하지 않음\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "from models import ContrastiveModel\n",
    "\n",
    "projection_dim =128\n",
    "arg = 'sep'\n",
    "# 'sep', 'mean', 'concat'\n",
    "\n",
    "if arg == \"concat\":\n",
    "    input_size = 2*projection_dim\n",
    "else:\n",
    "    input_size = projection_dim\n",
    "\n",
    "\n",
    "hidden_sizes = [128, 64]    # Hidden layer 크기\n",
    "output_size = 3672          # 클래스 수\n",
    "\n",
    "model = Mlp_classifier(input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss 및 Optimizer 설정\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()  # BCE 손실 함수\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------\n",
    "# 학습 - stage 2\n",
    "# ------------\n",
    "num_epochs = 10  # 학습 에포크 수\n",
    "model.to(device)\n",
    "stage1_model.to(device)\n",
    "\n",
    "from sklearn.metrics import hamming_loss  # 해밍 손실 계산\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # 에포크 동안 손실 누적\n",
    "\n",
    "    for batch in train_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_train = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_train.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        \n",
    "        if arg == 'sep' : \n",
    "            # Forward Pass\n",
    "            loss = 0\n",
    "            for emb in [emb1, emb2]:\n",
    "                pred = model(emb)  # MLP로 예측\n",
    "                part_loss = criterion(pred, y_train)  # 손실 계산\n",
    "                loss += part_loss\n",
    "                \n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        elif arg == 'mean':\n",
    "            emb = (emb1 + emb2) / 2\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "        elif arg == 'concat':\n",
    "            emb = torch.cat((emb1, emb2), dim=1)\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid arg value. Choose from 'sep', 'mean', 'concat'.\")\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)  # 평균 손실 계산\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Test (Validation) Loop\n",
    "    val_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred1 = []\n",
    "    y_pred2 = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch\n",
    "        clip_a, clip_b, target_value = clip_a.to(device), clip_b.to(device), target_value.to(device)\n",
    "        stage1_model.eval()\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_val = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_val.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        y_true.append(y_val.cpu().numpy())  # 리스트에 추가\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if arg == 'sep' : \n",
    "                # Forward Pass\n",
    "                pred1 = model(emb1)\n",
    "                y_pred1.append(pred1.cpu().numpy())  # 예측 결과 저장\n",
    "                pred2 = model(emb2)\n",
    "                y_pred2.append(pred2.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                    \n",
    "            elif arg == 'mean':\n",
    "                emb = (emb1 + emb2) / 2\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                \n",
    "                \n",
    "            elif arg == 'concat':\n",
    "                emb = torch.cat((emb1, emb2), dim=1)\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "    \n",
    "    # 리스트를 NumPy 배열로 변환하여 해밍 손실 계산\n",
    "    y_true = np.concatenate(y_true, axis=0)  # 실제 값\n",
    "    y_pred = np.concatenate(y_pred, axis=0)  # 예측 값\n",
    "\n",
    "    # 'sep'의 경우, pred1과 pred2에 대해 각각 해밍 손실을 계산한 후 평균을 구함\n",
    "    if arg == 'sep':\n",
    "        y_pred1 = np.concatenate(y_pred1, axis=0)\n",
    "        y_pred2 = np.concatenate(y_pred2, axis=0)\n",
    "        score = (hamming_loss(y_true, y_pred1) + hamming_loss(y_true, y_pred2)) / 2\n",
    "\n",
    "    else:\n",
    "        score = hamming_loss(y_true, y_pred)  # 해밍 손실 계산\n",
    "\n",
    "    print(f'Hamming Loss: {score:.4f}')\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-a27ad7a4e09c>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m# Stage 1 Embedding\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Stage 1은 학습하지 않음\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     emb1,emb2 \u001b[39m=\u001b[39m stage1_model(clip_a, clip_b)\n\u001b[1;32m     79\u001b[0m \u001b[39m# 원핫인코딩 y 집합 벡터\u001b[39;00m\n\u001b[1;32m     80\u001b[0m batch_size \u001b[39m=\u001b[39m target_value\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m# (batch_size는 train_loader에서 정의한 것과 같음)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1733'>1734</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1734'>1735</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1735'>1736</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1741'>1742</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1742'>1743</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1743'>1744</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1744'>1745</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1745'>1746</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1746'>1747</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1748'>1749</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1749'>1750</a>\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/project/src/models.py:32\u001b[0m, in \u001b[0;36mContrastiveModel.forward\u001b[0;34m(self, clip_a, clip_b)\u001b[0m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/models.py?line=28'>29</a>\u001b[0m     clip_b_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(clip_b_embeddings)\u001b[39m.\u001b[39mfloat()  \u001b[39m# NumPy -> Tensor 변환\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/models.py?line=30'>31</a>\u001b[0m \u001b[39m# Projection\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/models.py?line=31'>32</a>\u001b[0m projected_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection_head(clip_a_embeddings)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/models.py?line=32'>33</a>\u001b[0m projected_b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_head(clip_b_embeddings)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/models.py?line=34'>35</a>\u001b[0m \u001b[39m#projection 합치기\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1733'>1734</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1734'>1735</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1735'>1736</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1741'>1742</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1742'>1743</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1743'>1744</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1744'>1745</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1745'>1746</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1746'>1747</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1748'>1749</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1749'>1750</a>\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=247'>248</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=248'>249</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=249'>250</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=250'>251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1733'>1734</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1734'>1735</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1735'>1736</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1741'>1742</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1742'>1743</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1743'>1744</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1744'>1745</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1745'>1746</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1746'>1747</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1748'>1749</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1749'>1750</a>\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py?line=123'>124</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py?line=124'>125</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Mlp_classifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Hidden Layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid for multi-label classification\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "stage1_model = torch.load(\"/home/elicer/project/src/WNS_model.pth\")\n",
    "stage1_model.eval()  # Stage 1은 학습하지 않음\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "from models import ContrastiveModel\n",
    "\n",
    "projection_dim =128\n",
    "arg = 'sep'\n",
    "# 'sep', 'mean', 'concat'\n",
    "\n",
    "if arg == \"concat\":\n",
    "    input_size = 2*projection_dim\n",
    "else:\n",
    "    input_size = projection_dim\n",
    "\n",
    "\n",
    "hidden_sizes = [128, 64]    # Hidden layer 크기\n",
    "output_size = 3672          # 클래스 수\n",
    "\n",
    "model = Mlp_classifier(input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss 및 Optimizer 설정\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()  # BCE 손실 함수\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------\n",
    "# 학습 - stage 2\n",
    "# ------------\n",
    "num_epochs = 10  # 학습 에포크 수\n",
    "model.to(device)\n",
    "stage1_model.to(device)\n",
    "\n",
    "from sklearn.metrics import hamming_loss  # 해밍 손실 계산\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # 에포크 동안 손실 누적\n",
    "\n",
    "    for batch in train_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "        clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_train = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_train.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        \n",
    "        if arg == 'sep' : \n",
    "            # Forward Pass\n",
    "            loss = 0\n",
    "            for emb in [emb1, emb2]:\n",
    "                pred = model(emb)  # MLP로 예측\n",
    "                part_loss = criterion(pred, y_train)  # 손실 계산\n",
    "                loss += part_loss\n",
    "                \n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        elif arg == 'mean':\n",
    "            emb = (emb1 + emb2) / 2\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "        elif arg == 'concat':\n",
    "            emb = torch.cat((emb1, emb2), dim=1)\n",
    "            pred = model(emb)\n",
    "            loss = criterion(pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid arg value. Choose from 'sep', 'mean', 'concat'.\")\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)  # 평균 손실 계산\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Test (Validation) Loop\n",
    "    val_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred1 = []\n",
    "    y_pred2 = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        clip_a, clip_b, file_ids, target_value = batch\n",
    "        clip_a, clip_b= clip_a.to(device), clip_b.to(device)\n",
    "        stage1_model.eval()\n",
    "\n",
    "        # Stage 1 Embedding\n",
    "        with torch.no_grad():  # Stage 1은 학습하지 않음\n",
    "            emb1,emb2 = stage1_model(clip_a, clip_b)\n",
    "\n",
    "        # 원핫인코딩 y 집합 벡터\n",
    "        batch_size = target_value.size(0) # (batch_size는 train_loader에서 정의한 것과 같음)\n",
    "        y_val = torch.zeros((batch_size, output_size), device=device)  # 배치 크기에 맞는 텐서 \n",
    "        y_val.scatter_(1, target_value.unsqueeze(1), 1)  # target_value를 multi-hot 벡터로 변환\n",
    "        y_true.append(y_val.cpu().numpy())  # 리스트에 추가\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if arg == 'sep' : \n",
    "                # Forward Pass\n",
    "                pred1 = model(emb1)\n",
    "                y_pred1.append(pred1.cpu().numpy())  # 예측 결과 저장\n",
    "                pred2 = model(emb2)\n",
    "                y_pred2.append(pred2.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                    \n",
    "            elif arg == 'mean':\n",
    "                emb = (emb1 + emb2) / 2\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "\n",
    "                \n",
    "                \n",
    "            elif arg == 'concat':\n",
    "                emb = torch.cat((emb1, emb2), dim=1)\n",
    "                pred = model(emb)\n",
    "                y_pred.append(pred.cpu().numpy())  # 예측 결과 저장\n",
    "    \n",
    "    # 리스트를 NumPy 배열로 변환하여 해밍 손실 계산\n",
    "    y_true = np.concatenate(y_true, axis=0)  # 실제 값\n",
    "    y_pred = np.concatenate(y_pred, axis=0)  # 예측 값\n",
    "\n",
    "    # 'sep'의 경우, pred1과 pred2에 대해 각각 해밍 손실을 계산한 후 평균을 구함\n",
    "    if arg == 'sep':\n",
    "        y_pred1 = np.concatenate(y_pred1, axis=0)\n",
    "        y_pred2 = np.concatenate(y_pred2, axis=0)\n",
    "        score = (hamming_loss(y_true, y_pred1) + hamming_loss(y_true, y_pred2)) / 2\n",
    "\n",
    "    else:\n",
    "        score = hamming_loss(y_true, y_pred)  # 해밍 손실 계산\n",
    "\n",
    "    print(f'Hamming Loss: {score:.4f}')\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "No kernel connected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
