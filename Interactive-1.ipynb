{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ASTEncoder' object has no attribute 'set_train_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 16\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=54'>55</a>\u001b[0m \u001b[39m# 1. 모델과 옵티마이저 초기화\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=55'>56</a>\u001b[0m \u001b[39m######## 여기 만들어야 함!!!!!!!!!!!!\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39;49mset_train_mode()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=59'>60</a>\u001b[0m projection_dim \u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder, projection_dim \u001b[39m=\u001b[39m projection_dim)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ASTEncoder' object has no attribute 'set_train_mode'"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "projection_dim =128\n",
    "model = CotrastiveModel(ast_encoder, projection_dim = projection_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() got an unexpected keyword argument 'projection_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=59'>60</a>\u001b[0m projection_dim \u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder, projection_dim \u001b[39m=\u001b[39;49m projection_dim)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=63'>64</a>\u001b[0m \u001b[39m# 2. BERT 모델 로드 (가사 임베딩용)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:479\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=476'>477</a>\u001b[0m \u001b[39m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=477'>478</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(kwargs):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() got an unexpected keyword argument 'projection_dim'"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "projection_dim =128\n",
    "model = CotrastiveModel(ast_encoder, projection_dim = projection_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() got an unexpected keyword argument 'projection_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=59'>60</a>\u001b[0m projection_dim \u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder, projection_dim \u001b[39m=\u001b[39;49m projection_dim)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=63'>64</a>\u001b[0m \u001b[39m# 2. BERT 모델 로드 (가사 임베딩용)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:479\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=476'>477</a>\u001b[0m \u001b[39m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=477'>478</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(kwargs):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() got an unexpected keyword argument 'projection_dim'"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "projection_dim =128\n",
    "model = CotrastiveModel(ast_encoder, projection_dim = projection_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() got an unexpected keyword argument 'projection_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=59'>60</a>\u001b[0m projection_dim \u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder, projection_dim \u001b[39m=\u001b[39;49m projection_dim)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=63'>64</a>\u001b[0m \u001b[39m# 2. BERT 모델 로드 (가사 임베딩용)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:479\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=476'>477</a>\u001b[0m \u001b[39m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=477'>478</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(kwargs):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() got an unexpected keyword argument 'projection_dim'"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "projection_dim =128\n",
    "model = CotrastiveModel(ast_encoder, projection_dim = projection_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=63'>64</a>\u001b[0m \u001b[39m# 2. BERT 모델 로드 (가사 임베딩용)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=489'>490</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=490'>491</a>\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=491'>492</a>\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=492'>493</a>\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=493'>494</a>\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=494'>495</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=495'>496</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=489'>490</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=490'>491</a>\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=491'>492</a>\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=492'>493</a>\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=493'>494</a>\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=494'>495</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=495'>496</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=489'>490</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=490'>491</a>\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=491'>492</a>\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=492'>493</a>\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=493'>494</a>\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=494'>495</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=495'>496</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Restarted Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=489'>490</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=490'>491</a>\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=491'>492</a>\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=492'>493</a>\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=493'>494</a>\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=494'>495</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=495'>496</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=489'>490</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=490'>491</a>\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=491'>492</a>\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=492'>493</a>\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=493'>494</a>\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=494'>495</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=495'>496</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 19\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=56'>57</a>\u001b[0m ast_encoder \u001b[39m=\u001b[39m ASTEncoder()\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=57'>58</a>\u001b[0m ast_encoder\u001b[39m.\u001b[39mset_train_mode()\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=60'>61</a>\u001b[0m model \u001b[39m=\u001b[39m CotrastiveModel(ast_encoder)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=61'>62</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=479'>480</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=481'>482</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m--> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=484'>485</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=485'>486</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=486'>487</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=487'>488</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=489'>490</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=490'>491</a>\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=491'>492</a>\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=492'>493</a>\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=493'>494</a>\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=494'>495</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=495'>496</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------\n",
    "# dataloaders\n",
    "# ------------\n",
    "from data import create_contrastive_datasets, ContrastiveDataset\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "dataset_dir = \"/home/elicer/project/collect_data/mp3\"\n",
    "\n",
    "# 오디오 파일 경로 및 데이터셋 준비\n",
    "train_dataset, val_dataset = create_contrastive_datasets(dataset_dir, train_ratio=0.8)\n",
    "\n",
    "#Stage 1 학습 파라미터\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "target_column = 'Set Index'\n",
    "sim_set_dir = '/home/elicer/project/data/final_final.csv'\n",
    "\n",
    "# ContrastiveDataset으로 변환\n",
    "sample_rate = 44100 # [1, sample_rate*30]: 30초로 구간 설정\n",
    "train_contrastive_dataset = ContrastiveDataset(train_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "val_contrastive_dataset = ContrastiveDataset(val_dataset, sim_set_dir, target_column, input_shape=[1, sample_rate*30])\n",
    "\n",
    "# DataLoader로 배치 생성\n",
    "train_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last =True)\n",
    "val_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, shuffle=False, drop_last= True)\n",
    "# -> 한 배치의 구성 : clip_a, clip_b, file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "from models import CotrastiveModel\n",
    "from ast_encoder import ASTEncoder\n",
    "from loss import soft_info_nce_loss, info_nce_loss\n",
    "from loss_weight import generate_lyrics_embeddings, compute_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 모델과 옵티마이저 초기화\n",
    "######## 여기 만들어야 함!!!!!!!!!!!!\n",
    "ast_encoder = ASTEncoder()\n",
    "ast_encoder.set_train_mode()\n",
    "\n",
    "\n",
    "model = CotrastiveModel(ast_encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERT 모델 로드 (가사 임베딩용)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#3. 학습 모델 로드\n",
    "def train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = soft_info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                sim_ij=sim_ij,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    #모델 저장\n",
    "    torch.save(model, 'WNS_model.pth')\n",
    "        \n",
    "\n",
    "def train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            clip_a, clip_b, file_ids, target_value = batch  # 오디오와 file_ids 로드\n",
    "            clip_a, clip_b = clip_a.to(device), clip_b.to(device)\n",
    "\n",
    "            # 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\n",
    "            projected_a, projected_b = model(clip_a, clip_b)\n",
    "\n",
    "            audio_embeddings = torch.cat([projected_a, projected_b], dim=0)  # Combine along the batch dimension\n",
    "            print(audio_embeddings.shape)\n",
    "\n",
    "            # 2) 가사 임베딩 생성\n",
    "            lyrics_embeddings = generate_lyrics_embeddings(file_ids, bert_model, tokenizer, device)\n",
    "\n",
    "            # 3) 가사 임베딩들 간의 유사도 계산\n",
    "            sim_ij = compute_similarity(lyrics_embeddings.repeat(2, 1))        \n",
    "\n",
    "            # 4) 손실 계산\n",
    "            loss = info_nce_loss(\n",
    "                features=audio_embeddings,\n",
    "                batch_size=batch_size,\n",
    "                n_views=2,\n",
    "                temperature=0.5,\n",
    "                device=device\n",
    "            )\n",
    "            loss = loss.requires_grad_()\n",
    "\n",
    "            # 5) 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #모델 저장\n",
    "    torch.save(model, 'NS_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing index 389: empty range for randrange() (0, -525939, -525939)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CotrastiveModel.forward() missing 1 required positional argument: 'file_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 11\n\u001b[1;32m      <a href='file:///home/elicer/project/src/main.py?line=161'>162</a>\u001b[0m weighting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=162'>163</a>\u001b[0m \u001b[39mif\u001b[39;00m weighting:\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=163'>164</a>\u001b[0m     train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=164'>165</a>\u001b[0m     \u001b[39m#staage 2함수\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=165'>166</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=166'>167</a>\u001b[0m     train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size)\n",
      "\u001b[1;32m/home/elicer/project/src/main.py\u001b[0m in \u001b[0;36mline 18\u001b[0m, in \u001b[0;36mtrain_weighted_negative_sampling\u001b[0;34m(train_loader, model, optimizer, bert_model, tokenizer, device, num_epochs, batch_size)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=77'>78</a>\u001b[0m clip_a, clip_b \u001b[39m=\u001b[39m clip_a\u001b[39m.\u001b[39mto(device), clip_b\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=79'>80</a>\u001b[0m \u001b[39m# 1) 오디오 임베딩 생성 (오디오는 로스 계산에만 사용)\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/elicer/project/src/main.py?line=80'>81</a>\u001b[0m projected_a, projected_b \u001b[39m=\u001b[39m model(clip_a, clip_b)\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=82'>83</a>\u001b[0m audio_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([projected_a, projected_b], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Combine along the batch dimension\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/elicer/project/src/main.py?line=83'>84</a>\u001b[0m \u001b[39mprint\u001b[39m(audio_embeddings\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1733'>1734</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1734'>1735</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1735'>1736</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1741'>1742</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1742'>1743</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1743'>1744</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1744'>1745</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1745'>1746</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1746'>1747</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1748'>1749</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1749'>1750</a>\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: CotrastiveModel.forward() missing 1 required positional argument: 'file_id'"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# 학습 - stage 1\n",
    "# ------------\n",
    "\n",
    "\n",
    "\n",
    "#둘 중 하나는 빼고 돌리기 \n",
    "weighting = True\n",
    "if weighting:\n",
    "    train_weighted_negative_sampling(train_loader, model, optimizer, bert_model,tokenizer,device,num_epochs,batch_size)\n",
    "    #staage 2함수\n",
    "else:\n",
    "    train_pure_negative_sampling(train_loader,model,optimizer, bert_model,tokenizer, device, num_epochs,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
